{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PuckWorld_DiffQ_eps_1-0.1_0_model_params.pt  \u001b[0m\u001b[01;34msweep1\u001b[0m/\n",
      "PuckWorld_DiffQ_eps_1-0.1_0.npy\n",
      "Catcher_DiffQ_eps_1-0.1_0_model_params.pt  Catcher_DiffQ_eps_1-0.1_0.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "%ls results/control/puckworld/\n",
    "%ls results/control/catcher/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rewards_all': array([[0., 0., 0., ..., 0., 0., 0.]]), 'avg_reward_estimates': array([[ 0.        ,  0.        ,  0.        , ..., -0.00325839,\n",
      "        -0.00325844, -0.00325844]]), 'params': {'exp_name': 'Catcher_DiffQ_eps_1-0.1', 'env': 'Catcher', 'agent': 'DifferentialQlearningAgent', 'exp_parameters': {'num_runs': 1, 'num_max_steps': 1000000, 'save_model_params': True}, 'env_parameters': {'seed': 0}, 'agent_parameters': {'policy_type': 'egreedy', 'epsilon_start': 1.0, 'epsilon_end': 0.1, 'warmup_steps': 1000, 'decay_period': 10000, 'layer_sizes': [4, 5, 5, 2], 'er_buffer_capacity': 10000, 'batch_size': 32, 'steps_per_target_network_update': 1000, 'alpha': 0.00125, 'eta': 0.00125, 'seed': 0}}}\n"
     ]
    }
   ],
   "source": [
    "# d = np.load('results/control/puckworld/PuckWorld_DiffQ_eps_1-0.1_0.npy', allow_pickle=True).item()\n",
    "d = np.load('results/control/catcher/Catcher_DiffQ_eps_1-0.1_0.npy', allow_pickle=True).item()\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6711124583207032"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# win/loss ratio for Catcher\n",
    "np.sum(d['rewards_all'] == 1) / np.sum(d['rewards_all'] == -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ..., -0.00325839,\n",
       "        -0.00325844, -0.00325844]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['avg_reward_estimates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27565"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(d['rewards_all'] == 1) + np.sum(d['rewards_all'] == -1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:abi]",
   "language": "python",
   "name": "conda-env-abi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
