{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PuckWorld_DiffQ_eps_1-0.1_0_model_params.pt\n",
      "PuckWorld_DiffQ_eps_1-0.1_0.npy\n",
      "PuckWorld_RLearn_eps_1-0.1_0_model_params.pt\n",
      "PuckWorld_RLearn_eps_1-0.1_0.npy\n",
      "PuckWorld_RVIQ_test_0_model_params.pt\n",
      "PuckWorld_RVIQ_test_0.npy\n",
      "\u001b[0m\u001b[01;34msweep1\u001b[0m/\n",
      "Catcher_DiffQ_eps_1-0.1_0_model_params.pt\n",
      "Catcher_DiffQ_eps_1-0.1_0.npy\n",
      "Catcher_DiffQ_eps_1-0.1_sweep_0_model_params.pt\n",
      "Catcher_DiffQ_eps_1-0.1_sweep_0.npy\n",
      "Catcher_DiffQ_eps_1-0.1_sweep_10_model_params.pt\n",
      "Catcher_DiffQ_eps_1-0.1_sweep_10.npy\n",
      "Catcher_DiffQ_eps_1-0.1_sweep_11_model_params.pt\n",
      "Catcher_DiffQ_eps_1-0.1_sweep_11.npy\n",
      "Catcher_DiffQ_eps_1-0.1_sweep_12_model_params.pt\n",
      "Catcher_DiffQ_eps_1-0.1_sweep_12.npy\n",
      "Catcher_DiffQ_eps_1-0.1_sweep_13_model_params.pt\n",
      "Catcher_DiffQ_eps_1-0.1_sweep_13.npy\n",
      "Catcher_DiffQ_eps_1-0.1_sweep_14_model_params.pt\n",
      "Catcher_DiffQ_eps_1-0.1_sweep_14.npy\n",
      "Catcher_DiffQ_eps_1-0.1_sweep_15_model_params.pt\n",
      "Catcher_DiffQ_eps_1-0.1_sweep_15.npy\n",
      "Catcher_DiffQ_eps_1-0.1_sweep_1_model_params.pt\n",
      "Catcher_DiffQ_eps_1-0.1_sweep_1.npy\n",
      "Catcher_DiffQ_eps_1-0.1_sweep_2_model_params.pt\n",
      "Catcher_DiffQ_eps_1-0.1_sweep_2.npy\n",
      "Catcher_DiffQ_eps_1-0.1_sweep_3_model_params.pt\n",
      "Catcher_DiffQ_eps_1-0.1_sweep_3.npy\n",
      "Catcher_DiffQ_eps_1-0.1_sweep_4_model_params.pt\n",
      "Catcher_DiffQ_eps_1-0.1_sweep_4.npy\n",
      "Catcher_DiffQ_eps_1-0.1_sweep_5_model_params.pt\n",
      "Catcher_DiffQ_eps_1-0.1_sweep_5.npy\n",
      "Catcher_DiffQ_eps_1-0.1_sweep_6_model_params.pt\n",
      "Catcher_DiffQ_eps_1-0.1_sweep_6.npy\n",
      "Catcher_DiffQ_eps_1-0.1_sweep_7_model_params.pt\n",
      "Catcher_DiffQ_eps_1-0.1_sweep_7.npy\n",
      "Catcher_DiffQ_eps_1-0.1_sweep_8_model_params.pt\n",
      "Catcher_DiffQ_eps_1-0.1_sweep_8.npy\n",
      "Catcher_DiffQ_eps_1-0.1_sweep_9_model_params.pt\n",
      "Catcher_DiffQ_eps_1-0.1_sweep_9.npy\n",
      "Catcher_RVIQ_eps_1-0.1_0_model_params.pt\n",
      "Catcher_RVIQ_eps_1-0.1_0.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "%ls results/control/puckworld/\n",
    "%ls results/control/catcher/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rewards_all': array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0., -1.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ...,\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), 'avg_reward_estimates': array([[ 0.        ,  0.        ,  0.        , ..., -0.0035402 ,\n",
      "        -0.02799588,  0.04646262],\n",
      "       [ 0.        ,  0.        ,  0.        , ...,  0.01068133,\n",
      "        -0.02121469, -0.07897355],\n",
      "       [ 0.        ,  0.        ,  0.        , ...,  0.00900542,\n",
      "        -0.08923353,  0.02189127],\n",
      "       ...,\n",
      "       [ 0.        ,  0.        ,  0.        , ...,  0.03687679,\n",
      "         0.00192294, -0.01951876],\n",
      "       [ 0.        ,  0.        ,  0.        , ..., -0.01095064,\n",
      "        -0.03256762,  0.01257534],\n",
      "       [ 0.        ,  0.        ,  0.        , ...,  0.01315641,\n",
      "         0.01300413,  0.01148234]]), 'params': {'exp_name': 'Catcher_DiffQ_eps_1-0.1_sweep', 'env': 'Catcher', 'agent': 'DifferentialQlearningAgent', 'exp_parameters': {'num_runs': 30, 'num_max_steps': 80000, 'save_model_params': True}, 'env_parameters': {'seed': 29}, 'agent_parameters': {'policy_type': 'egreedy', 'epsilon_start': 1.0, 'epsilon_end': 0.1, 'warmup_steps': 1000, 'decay_period': 4000, 'layer_sizes': [4, 5, 5, 2], 'er_buffer_capacity': 80000, 'batch_size': 32, 'steps_per_target_network_update': 1000, 'alpha': 0.1, 'eta': 10, 'seed': 29}}}\n"
     ]
    }
   ],
   "source": [
    "# d = np.load('results/control/puckworld/PuckWorld_DiffQ_eps_1-0.1_0.npy', allow_pickle=True).item()\n",
    "d = np.load('results/control/catcher/Catcher_DiffQ_eps_1-0.1_sweep_3.npy', allow_pickle=True).item()\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4053978674561799"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# win/loss ratio for Catcher\n",
    "np.sum(d['rewards_all'] == 1) / np.sum(d['rewards_all'] == -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exp_name': 'Catcher_DiffQ_eps_1-0.1_sweep',\n",
       " 'env': 'Catcher',\n",
       " 'agent': 'DifferentialQlearningAgent',\n",
       " 'exp_parameters': {'num_runs': 30,\n",
       "  'num_max_steps': 80000,\n",
       "  'save_model_params': True},\n",
       " 'env_parameters': {'seed': 29},\n",
       " 'agent_parameters': {'policy_type': 'egreedy',\n",
       "  'epsilon_start': 1.0,\n",
       "  'epsilon_end': 0.1,\n",
       "  'warmup_steps': 1000,\n",
       "  'decay_period': 4000,\n",
       "  'layer_sizes': [4, 5, 5, 2],\n",
       "  'er_buffer_capacity': 80000,\n",
       "  'batch_size': 32,\n",
       "  'steps_per_target_network_update': 1000,\n",
       "  'alpha': 0.0001,\n",
       "  'eta': 10,\n",
       "  'seed': 29}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65803"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(d['rewards_all'] == 1) + np.sum(d['rewards_all'] == -1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:abi]",
   "language": "python",
   "name": "conda-env-abi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
